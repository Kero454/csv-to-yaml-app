# @package _global_

model:
  type: autoformer
  retrain: true
ts:
  name: autotransformer_1
  version: 1
  enrich: []
  use_covariates: true
  past_variables: []
  future_variables: []
  static_variables: []
train_config:
  batch_size: 32
  max_epochs: 20
model_configs:
  activation: torch.nn.PReLU
  d_model: 4
  dropout_rate: 0.5
  factor: 5
  hidden_size: 12
  kernel_size: 3
  label_len: 4
  loss_type: l1
  n_head: 2
  n_layer_decoder: 2
  n_layer_encoder: 2
  optim: torch.optim.Adam
  persistence_weight: 0.01
